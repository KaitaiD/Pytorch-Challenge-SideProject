{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchtext\n",
    "\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and basic re-transformatting \n",
    "with open('reviews.json', 'r') as read_file:\n",
    "    json_yelp = json.load(read_file)\n",
    "    \n",
    "df = json_normalize(json_yelp)['stars', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the test and train datasets\n",
    "test_size = 0.2\n",
    "\n",
    "num_data = len(df)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * num_data))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_df = df[train_idx, :]\n",
    "test_df = df[test_idx, :]\n",
    "\n",
    "train_df.to_csv('train_dataset.csv')  # save it for future use\n",
    "test_df.to_csv('test_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize, fix_length=70)\n",
    "target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "\n",
    "train = torchtext.data.TabularDataset(path='mydir/train_dataset.csv', format='csv',\n",
    "                                      fields={'text': ('text',text),\n",
    "                                              'stars': ('target',target)})\n",
    "test = torchtext.data.TabularDataset(path='mydir/test_dataset.csv', format='csv',\n",
    "                                     fields={'text': ('text', text)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "text.build_vocab(train, test, min_freq=3)\n",
    "qid.build_vocab(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained language model\n",
    "glove = torchtext.vocab.Vectors('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "tqdm_notebook().pandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.vocab.set_vectors(glove.stoi, glove.vectors, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network for the text CNN\n",
    "class TextCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, lm, padding_idx, static=True, kernel_num=128, fixed_length=50, kernel_size=[2, 5, 10], dropout=0.2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding.from_pretrained(lm)\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1, kernel_num, (i, self.embedding.embedding_dim)) for i in kernel_size])\n",
    "        self.maxpools = [nn.MaxPool2d((fixed_length+1-i,1)) for i in kernel_size]\n",
    "        self.fc = nn.Linear(len(kernel_size)*kernel_num, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input).unsqueeze(1)  # B x Ci x H x W\n",
    "        x = [self.maxpools[i](torch.tanh(cov(x))).squeeze(3).squeeze(2) for i, cov in enumerate(self.conv)]  # B x Kn\n",
    "        x = torch.cat(x, dim=1)  # B x Kn * len(Kz)\n",
    "        y = self.fc(self.dropout(x))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training process and strategy\n",
    "def search_best_f1(true, pred):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in np.arange(0.1, 0.501, 0.01):\n",
    "        tmp[1] = f1_score(true, np.array(pred)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    return tmp[2], delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, loss_func, optimizer, train_iter, val_iter):\n",
    "    e = 0\n",
    "    \n",
    "    while e < epoch:\n",
    "        train_iter.init_epoch()\n",
    "        losses, preds, true = [], [], []\n",
    "        for train_batch in tqdm(list(iter(train_iter)), 'epcoh {} training'.format(e)):\n",
    "            model.train()\n",
    "            x = train_batch.text.cuda()\n",
    "            y = train_batch.target.type(torch.Tensor).cuda()\n",
    "            true.append(train_batch.target.numpy())\n",
    "            model.zero_grad()\n",
    "            pred = model.forward(x).view(-1)\n",
    "            loss = loss_function(pred, y)\n",
    "            preds.append(torch.sigmoid(pred).cpu().data.numpy())\n",
    "            losses.append(loss.cpu().data.numpy())\n",
    "            loss.backward()\n",
    "#             clip_grad_norm_(model.parameters(), 2)\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            val_loss = []\n",
    "            val_preds = []\n",
    "            val_true =[]\n",
    "            for val_batch in tqdm(val_iter, 'epcoh {} validating'.format(e)):\n",
    "                val_x = val_batch.text.cuda()\n",
    "                val_y = val_batch.target.type(torch.Tensor).cuda()\n",
    "                val_true.append(val_batch.target.numpy())\n",
    "                val_pred = model.forward(val_x).view(-1)\n",
    "                val_preds.append(torch.sigmoid(val_pred).cpu().data.numpy())\n",
    "                val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())\n",
    "            train_f1, alpha_train = search_best_f1([j for i in true for j in i], [j for i in preds for j in i])\n",
    "            val_f1, alpha_val = search_best_f1([j for i in val_true for j in i], [j for i in val_preds for j in i])\n",
    "            print('epcoh {:02} - train_loss {:.4f} - val_loss {:.4f} '\n",
    "                      '- train f1 {:.4f} - val f1 {:.4f}'.format(\n",
    "                            e, np.mean(losses), np.mean(val_loss),\n",
    "                            train_f1, val_f1))\n",
    "                \n",
    "        e += 1\n",
    "    return alpha_val\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the batch set and also complete the train/validation split\n",
    "random.seed(1234)\n",
    "train, val = train.split(split_ratio=0.8, random_state=random.getstate())\n",
    "batch_size = 512\n",
    "train_iter = torchtext.data.BucketIterator(dataset=train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               sort=False)\n",
    "val_iter = list(torchtext.data.BucketIterator(dataset=val,\n",
    "                                             batch_size=batch_size,\n",
    "                                             train=False,\n",
    "                                             sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network init\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    for name, w in model.named_parameters():\n",
    "        if not exclude in name:\n",
    "            if 'weight' in name:\n",
    "                if method is 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method is 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0.0)\n",
    "            else: \n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, ignore='embedding'):\n",
    "    total = 0\n",
    "    for name, w in model.named_parameters():\n",
    "        if not ignore or ignore not in name:\n",
    "            total += w.nelement()\n",
    "            print('{} : {}  {} parameters'.format(name, w.shape, w.nelement()))\n",
    "    print('-------'*4)\n",
    "    print('Total {} parameters'.format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the CNN model for text classification \n",
    "text.fix_length = 70\n",
    "model = TextCNN(text.vocab.vectors, padding_idx=text.vocab.stoi[text.pad_token], kernel_size=[1, 2, 3, 5], kernel_num=128, static=False, fixed_length=text.fix_length, dropout=0.1).cuda()\n",
    "init_network(model)\n",
    "# choose ideal optimizer and loss function for the task\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "print_model(model, ignore=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = training(3, model, loss_function, optimizer, train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_list):\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_list:\n",
    "            model.eval()\n",
    "            x = test_batch.text.cuda()\n",
    "            pred += torch.sigmoid(model.forward(x).view(-1)).cpu().data.numpy().tolist()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(torchtext.data.BucketIterator(dataset=test,\n",
    "                                    batch_size=batch_size,\n",
    "                                    sort=False,\n",
    "                                    train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the review stars and put the result in a dataframe\n",
    "preds = predict(model, test_list)\n",
    "sub = pd.DataFrame()\n",
    "sub['qid'] = [qid.vocab.itos[j] for i in test_list for j in i.qid.view(-1).numpy()]\n",
    "sub['prediction'] = (preds > alpha).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the prediction result\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
